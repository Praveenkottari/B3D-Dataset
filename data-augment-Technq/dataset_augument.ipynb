{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d28852f2-b3a1-449f-8172-c1e5ac8798d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2615fd-8a44-46da-9eed-162f4d2701ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "source_directory = \"../../dataset/curated-dataset\"\n",
    "destination_directory = \"../../dataset/augmented\"\n",
    " \n",
    "# Ensure destination directory exists\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc820187-6260-44ca-904c-89bdeb5f808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation transforms\n",
    "\n",
    "augmentation_transforms = [\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    # transforms.RandomVerticalFlip(p=1),\n",
    "    #transforms.RandomRotation(degrees=270),\n",
    "\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    #transforms.RandomResizedCrop(size=(512, 512), scale=(0.8, 1.2))\n",
    "    #transforms.RandomPerspective(),\n",
    "\n",
    "\n",
    "    transforms.ColorJitter(brightness=0.6),\n",
    "    transforms.ColorJitter(contrast=0.6),\n",
    "    #transforms.ElasticTransform(alpha=250.0),\n",
    "    #transforms.RandomAdjustSharpness(sharpness_factor=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6e746b-53d7-4a5c-bed5-273561458787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and augment images\n",
    "def augment_images(class_dir, dest_class_dir, required_count):\n",
    "    image_files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "\n",
    "    # Shuffle the image files\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    original_count = len(image_files)\n",
    "    augmentations_per_image = (required_count - original_count) // original_count\n",
    "    \n",
    "    for i, image_file in enumerate(tqdm(image_files, desc=f\"Processing {class_dir}\")):\n",
    "        img_path = os.path.join(class_dir, image_file)\n",
    "        img = Image.open(img_path)\n",
    "        save_image(transforms.ToTensor()(img), os.path.join(dest_class_dir, f\"{i}.jpg\"))\n",
    "        \n",
    "        # Apply augmentations\n",
    "        for j in range(augmentations_per_image):\n",
    "            transform = transforms.Compose([\n",
    "                random.choice(augmentation_transforms),\n",
    "                transforms.Resize((512, 512))\n",
    "            ])\n",
    "            augmented_img = transform(img)\n",
    "            save_image(transforms.ToTensor()(augmented_img), os.path.join(dest_class_dir, f\"{i}_{j}.jpg\"))\n",
    "    \n",
    "    # If needed, add additional augmentations to reach the required count\n",
    "    augmented_count = len(os.listdir(dest_class_dir))\n",
    "    while augmented_count < required_count:\n",
    "        for image_file in image_files:\n",
    "            img_path = os.path.join(class_dir, image_file)\n",
    "            img = Image.open(img_path)\n",
    "            transform = transforms.Compose([\n",
    "                random.choice(augmentation_transforms),\n",
    "                transforms.Resize((512, 512))\n",
    "            ])\n",
    "            augmented_img = transform(img)\n",
    "            save_image(transforms.ToTensor()(augmented_img), os.path.join(dest_class_dir, f\"extra_{augmented_count}.jpg\"))\n",
    "            augmented_count += 1\n",
    "            if augmented_count >= required_count:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5fe64-9873-4a20-b289-0b974cb3f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../../dataset/curated-dataset/minor_crack: 100%|████████████| 624/624 [00:24<00:00, 25.81it/s]\n",
      "Processing ../../dataset/curated-dataset/peeling: 100%|████████████████| 520/520 [00:19<00:00, 26.64it/s]\n",
      "Processing ../../dataset/curated-dataset/plain: 100%|██████████████████| 600/600 [00:22<00:00, 26.76it/s]\n",
      "Processing ../../dataset/curated-dataset/stain: 100%|██████████████████| 521/521 [00:20<00:00, 25.86it/s]\n",
      "Processing ../../dataset/curated-dataset/algae:  31%|█████▋            | 195/620 [00:08<00:18, 22.47it/s]"
     ]
    }
   ],
   "source": [
    " \n",
    "# Process each class\n",
    "for class_name in os.listdir(source_directory):\n",
    "    class_dir = os.path.join(source_directory, class_name)\n",
    "    dest_class_dir = os.path.join(destination_directory, class_name)\n",
    "    os.makedirs(dest_class_dir, exist_ok=True)\n",
    "    augment_images(class_dir, dest_class_dir, 2000)\n",
    " \n",
    "print(\"Data augmentation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62cf221-a9b2-4de5-9677-b169434f3f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3349b2d-83b5-4ff0-87f9-30ef095c5fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
